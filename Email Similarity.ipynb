{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Similarity\n",
    "\n",
    "In this project, you will use scikit-learn's Naive Bayes implementation on several different datasets. By reporting the accuracy of the classifier, we can find which datasets are harder to distinguish.\n",
    "\n",
    "For example, how difficult do you think it is to distinguish the difference between emails about hockey and emails about soccer? \n",
    "\n",
    "How hard is it to tell the difference between emails about hockey and emails about tech? \n",
    "\n",
    "In this project, weâ€™ll find out exactly how difficult those two tasks are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Data\n",
    "We've imported a dataset of emails from scikit-learn's datasets. All of these emails are tagged based on their content.\n",
    "\n",
    "Print emails.target_names to see the different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "emails = fetch_20newsgroups()\n",
    "emails.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're interested in seeing how effective our Naive Bayes classifier is at telling the difference between a baseball email and a hockey email. We can select the categories of articles we want from __fetch_20newsgroups__ by adding the parameter categories.\n",
    "\n",
    "In the function call, set categories equal to the list ['rec.sport.baseball', 'rec.sport.hockey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = fetch_20newsgroups(categories = ['rec.sport.baseball', 'rec.sport.hockey'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of these emails.\n",
    "\n",
    "All of the emails are stored in a list called emails.data. Print the email at index 5 in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: mmb@lamar.ColoState.EDU (Michael Burger)\\nSubject: More TV Info\\nDistribution: na\\nNntp-Posting-Host: lamar.acns.colostate.edu\\nOrganization: Colorado State University, Fort Collins, CO  80523\\nLines: 36\\n\\nUnited States Coverage:\\nSunday April 18\\n  N.J./N.Y.I. at Pittsburgh - 1:00 EDT to Eastern Time Zone\\n  ABC - Gary Thorne and Bill Clement\\n\\n  St. Louis at Chicago - 12:00 CDT and 11:00 MDT - to Central/Mountain Zones\\n  ABC - Mike Emerick and Jim Schoenfeld\\n\\n  Los Angeles at Calgary - 12:00 PDT and 11:00 ADT - to Pacific/Alaskan Zones\\n  ABC - Al Michaels and John Davidson\\n\\nTuesday, April 20\\n  N.J./N.Y.I. at Pittsburgh - 7:30 EDT Nationwide\\n  ESPN - Gary Thorne and Bill Clement\\n\\nThursday, April 22 and Saturday April 24\\n  To Be Announced - 7:30 EDT Nationwide\\n  ESPN - To Be Announced\\n\\n\\nCanadian Coverage:\\n\\nSunday, April 18\\n  Buffalo at Boston - 7:30 EDT Nationwide\\n  TSN - ???\\n\\nTuesday, April 20\\n  N.J.D./N.Y. at Pittsburgh - 7:30 EDT Nationwide\\n  TSN - ???\\n\\nWednesday, April 21\\n  St. Louis at Chicago - 8:30 EDT Nationwide\\n  TSN - ???\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails.data[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the labels can be found in the list emails.target. Print the label of the email at index 5.\n",
    "\n",
    "The labels themselves are numbers, but those numbers correspond to the label names found at emails.target_names.\n",
    "\n",
    "Is this a baseball email or a hockey email?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails.target[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rec.sport.hockey'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails.target_names[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Training and Test Sets\n",
    "We now want to split our data into training and test sets. Change the name of your variable from __emails__ to __train_emails.__ Add these three parameters to the function call:\n",
    "\n",
    "    . subset='train'\n",
    "    . shuffle = True\n",
    "    . random_state = 108\n",
    "Adding the __random_state__ parameter will make sure that every time you run the code, your dataset is split in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emails = fetch_20newsgroups(categories = ['rec.sport.baseball', 'rec.sport.hockey'], \n",
    "                                    subset = 'train',\n",
    "                                    shuffle = True,\n",
    "                                    random_state = 108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: smorris@venus.lerc.nasa.gov (Ron Morris )\\nSubject: Murray as GM  (was: Wings will win\\nOrganization: NASA Lewis Research Center\\nLines: 37\\nDistribution: world\\nNNTP-Posting-Host: venus.lerc.nasa.gov\\nNews-Software: VAX/VMS VNEWS 1.41    \\n\\nIn article <1993Apr19.204348.8254@sol.UVic.CA>, gballent@hudson.UVic.CA writes...\\n> \\n>In article 735249453@vela.acs.oakland.edu, ragraca@vela.acs.oakland.edu (Randy A. Graca) writes:\\n> \\n>>are predicting).  Although I think Bryan Murray is probably the best GM\\n>>I have ever seen in hockey\\n> \\n>How do you figure that??  When Bryan Murray took over the Wings they were\\n>a pretty good team that was contending for the Stanley Cup but looked\\n>unlikely to win it.  Now they are a pretty good team that is contending for\\n>the Stanley Cup but looks unlikely to win it.  A truly great GM would\\n>have been able to make the moves to push the team to the upper echelon\\n>of the NHL and maybe win the Stanley Cup.  A good GM (like Murray) can\\n\\nI think Murray has done a great job.  He\\'s picked up Ciccarelli,\\nSheppard, Ysebaert, Howe, Coffey, and Riendeau (plus some depth players) \\nwithout giving up anything the Wings needed or any of his top prospects.\\nAll of this in three years.  Has anyone done better?\\n\\nThe year before he took over, the Wings didn\\'t even make the playoffs.\\nThere was about a year and a half during Demers\\' stint that the Wings\\ndid OK, but that was due to Demers\\' motavational skills and clutch\\nand grab style.  They didn\\'t have much talent.\\n\\nGerald, Murray wasn\\'t responsible for Primeau (although I\\'m not\\nready to admit that\\'s a horrible pick).  They hired him after the\\ndraft (which has never made sense to me).  His first pick was\\nLapointe.\\n\\nRon \\n\\n**********\\n\"And one of my major goals is to leave the next president a new set\\nof things to worry about.  I\\'m getting bored reading the same problems\\nin the paper, decade after decade.  I want people to have to deal\\nwith new problems.\"\\n                    ... President Bill Clinton   2-4-93\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_emails.data[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create another variable named __test_emails__ and set it equal to fetch_20newsgroups. The parameters of the function should be the same as before except subset should now be __'test'.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emails = fetch_20newsgroups(categories = ['rec.sport.baseball', 'rec.sport.hockey'], \n",
    "                                    subset = 'test',\n",
    "                                    shuffle = True,\n",
    "                                    random_state = 108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: lws@eembox.ncku.edu.tw (WenHsiang Lin)\\nSubject: Stats question\\nOrganization: National Cheng Kung University\\nLines: 5\\n\\n\\n\\tI am just wondering whether the official MLB stats includes \\nIntentional Walks in the BB category or not?\\n\\nWenHsiang Lin\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_emails.data[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Words\n",
    "\n",
    "We want to transform these emails into lists of word counts. The CountVectorizer class makes this easy for us.\n",
    "\n",
    "Create a CountVectorizer object and name it counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tell counter what possible words can exist in our emails. counter has a .fit() a function that takes a list of all your data.\n",
    "\n",
    "Call .fit() with __test_emails.data + train_emails.data__ as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.fit(test_emails.data + train_emails.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make a list of the counts of our words in our __training set.__\n",
    "\n",
    "Create a variable named __train_counts.__ Set it equal to counter's __transform function__ using __train_emails.data__ as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counts = counter.transform(train_emails.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1197x23714 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 174038 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also make a variable named test_counts. This should be the same function call as before, but use test_emails.data as the parameter of transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_counts = counter.transform(test_emails.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<796x23714 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 115794 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Naive Bayes Classifier\n",
    "\n",
    "Let's now make a Naive Bayes classifier that we can train and test on. Create a MultinomialNB object named classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call classifier's .fit() function. __.fit()__ takes two parameters. The first should be our __training set,__ which for us is __train_counts.__ The second should be the __labels__ associated with the __training emails.__ Those are found in __train_emails.target.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(train_counts, train_emails.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Naive Bayes Classifier by printing classifier's __.score() function.__ _.score()_ takes the __test set and the test labels__ as parameters.\n",
    "\n",
    ".score() returns the accuracy of the classifier on the test data. Accuracy measures the percentage of classifications a classifier correctly made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9723618090452262"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(test_counts, test_emails.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Other Datasets\n",
    "Our classifier does a pretty good job distinguishing between soccer emails and hockey emails. But let's see how it does with emails about really different topics.\n",
    "\n",
    "Find where you create train_emails and test_emails. Change the categories to be ['comp.sys.ibm.pc.hardware','rec.sport.hockey'].\n",
    "\n",
    "Did your classifier do a better or worse job on these two datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9974715549936789"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_emails = fetch_20newsgroups(categories = ['comp.sys.ibm.pc.hardware', 'rec.sport.hockey'], \n",
    "                                    subset = 'train',\n",
    "                                    shuffle = True,\n",
    "                                    random_state = 108)\n",
    "\n",
    "test_emails = fetch_20newsgroups(categories = ['comp.sys.ibm.pc.hardware', 'rec.sport.hockey'], \n",
    "                                    subset = 'test',\n",
    "                                    shuffle = True,\n",
    "                                    random_state = 108)\n",
    "counter = CountVectorizer()\n",
    "counter.fit(test_emails.data + train_emails.data)\n",
    "train_counts = counter.transform(train_emails.data)\n",
    "test_counts = counter.transform(test_emails.data)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_counts, train_emails.target)\n",
    "classifier.score(test_counts, test_emails.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "The classifier was 99% accurate when trying to classify hockey and tech emails.\n",
    "\n",
    "This is better than when it was trying to classify hockey and soccer emails. This makes sense â€” emails about sports probably share more words in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
